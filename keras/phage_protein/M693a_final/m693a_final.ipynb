{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved matrices\n",
    "import pickle\n",
    "train_X=pickle.load(open( \"../dipep_new_train_x.p\", \"rb\" ) )\n",
    "test_X=pickle.load(open( \"../dipep_new_test_x.p\", \"rb\" ) )\n",
    "train_Y=pickle.load(open( \"../dipep_new_train_y.p\", \"rb\" ) )\n",
    "test_Y=pickle.load(open( \"../dipep_new_test_y.p\", \"rb\" ) )\n",
    "mean_arr=pickle.load(open( \"../dipep_new_mean.p\", \"rb\" ) )\n",
    "mean_std=pickle.load(open( \"../dipep_new_std.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras and numpy\n",
    "import numpy\n",
    "from scipy import linalg as LA\n",
    "from scipy.optimize import fminbound as OPT\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this list the devices, just making sure there is a GPU present, you might be fine with no GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "f1=open('./testfile', 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_f(lr=0.001, dd1=0.2, dd2=0.2, dd3=0.2, vv=0):\n",
    "    #opt=Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    opt=SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(402, input_dim=402, kernel_initializer='random_uniform',activation='relu'))\n",
    "    model.add(Dropout(dd1))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dropout(dd2))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dropout(dd3))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    hist=model.fit(train_X, train_Y,verbose=vv, epochs=50, batch_size=200)\n",
    "    #scores = model.evaluate(test_X, test_Y, verbose=vv)\n",
    "    arr_pred=model.predict(test_X)\n",
    "    #if vv:\n",
    "    #    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    keras.backend.clear_session()\n",
    "    return LA.norm(arr_pred-test_Y)\n",
    "    #return scores[1]\n",
    "    #return hist;\n",
    "    \n",
    "def evaluate_f_v(VV):\n",
    "    val=evaluate_f(lr=VV[0], dd1=VV[1], dd2=VV[2], dd3=VV[3], vv=0)\n",
    "    return val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_f_acc(lr=0.001, dd1=0.2, dd2=0.2, dd3=0.2, vv=0):\n",
    "    #opt=Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    opt=SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(402, input_dim=402, kernel_initializer='random_uniform',activation='relu'))\n",
    "    model.add(Dropout(dd1))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dropout(dd2))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dropout(dd3))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    hist=model.fit(train_X, train_Y,verbose=vv, epochs=50, batch_size=200)\n",
    "    scores = model.evaluate(test_X, test_Y, verbose=vv)\n",
    "    #arr_pred=model.predict(test_X)\n",
    "    #if vv:\n",
    "    #    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    keras.backend.clear_session()\n",
    "    #return LA.norm(arr_pred-test_Y)\n",
    "    return scores[1]\n",
    "    #return hist;\n",
    "    \n",
    "def evaluate_f_acc_v(VV):\n",
    "    val=evaluate_f_acc(lr=VV[0], dd1=VV[1], dd2=VV[2], dd3=VV[3], vv=0)\n",
    "    return val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_evaluate_f(lr=0.001, dd1=0.2, dd2=0.2, dd3=0.2, vv=0):\n",
    "    f1=evaluate_f(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f2=evaluate_f(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f3=evaluate_f(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f4=evaluate_f(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f5=evaluate_f(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    return numpy.average([f1,f2,f3,f4,f5])\n",
    "\n",
    "def avg_evaluate_f_v(VV):\n",
    "    val=avg_evaluate_f(lr=VV[0], dd1=VV[1], dd2=VV[2], dd3=VV[3], vv=0)\n",
    "    return val\n",
    "\n",
    "def avg_evaluate_f_acc(lr=0.001, dd1=0.2, dd2=0.2, dd3=0.2, vv=0):\n",
    "    f1=evaluate_f_acc(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f2=evaluate_f_acc(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f3=evaluate_f_acc(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f4=evaluate_f_acc(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    f5=evaluate_f_acc(lr=lr, dd1=dd1, dd2=dd3, dd3=dd3, vv=vv)\n",
    "    return numpy.average([f1,f2,f3,f4,f5])\n",
    "\n",
    "def avg_evaluate_f_acc_v(VV):\n",
    "    val=avg_evaluate_f_acc(lr=VV[0], dd1=VV[1], dd2=VV[2], dd3=VV[3], vv=0)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_G(lr=0.001, dd1=0.2, dd2=0.2, dd3=0.2,h=0.001, k=0.0001):\n",
    "    df_lr=((avg_evaluate_f(lr=lr+k,dd1=dd1, dd2=dd2, dd3=dd3, vv=0)-\n",
    "            avg_evaluate_f(lr=lr-k,dd1=dd1, dd2=dd2, dd3=dd3, vv=0))/(2*k))\n",
    "    df_dd1=((avg_evaluate_f(dd1=dd1+h,dd2=dd2, dd3=dd3, vv=0,lr=lr)-\n",
    "             avg_evaluate_f(dd1=dd1-h,dd2=dd2, dd3=dd3, vv=0,lr=lr))/(2*h))\n",
    "    df_dd2=((avg_evaluate_f(dd2=dd2+h,dd1=dd1, dd3=dd3, vv=0,lr=lr)-\n",
    "             avg_evaluate_f(dd2=dd2-h,dd1=dd1, dd3=dd3, vv=0,lr=lr))/(2*h))\n",
    "    df_dd3=((avg_evaluate_f(dd3=dd3+h,dd1=dd1, dd2=dd2, vv=0,lr=lr)-\n",
    "             avg_evaluate_f(dd3=dd3-h,dd1=dd1, dd2=dd2, vv=0,lr=lr))/(2*h))\n",
    "    G=numpy.array([df_lr,df_dd1,df_dd2,df_dd3],dtype=numpy.float)\n",
    "    return G\n",
    "\n",
    "def evaluate_G_v(VV):\n",
    "    val=evaluate_G(lr=VV[0], dd1=VV[1], dd2=VV[2], dd3=VV[3])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_H(ff,lr=0.001, dd1=0.2, dd2=0.2, dd3=0.2,h=0.001, k=0.0001,vv=0):\n",
    "    \n",
    "    df_lr_lr=((avg_evaluate_f(lr=lr+2*k, dd1=dd1, dd2=dd2, dd3=dd3,vv=vv)-2*ff+\n",
    "               avg_evaluate_f(lr=lr-2*k, dd1=dd1, dd2=dd2, dd3=dd3,vv=vv))/(4*k*k))\n",
    "    df_dd1_dd1=((avg_evaluate_f(lr=lr, dd1=dd1+2*h, dd2=dd2, dd3=dd3,vv=vv)-2*ff+\n",
    "               avg_evaluate_f(lr=lr, dd1=dd1-2*h, dd2=dd2, dd3=dd3,vv=vv))/(4*h*h))\n",
    "    df_dd2_dd2=((avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2+2*h, dd3=dd3,vv=vv)-2*ff+\n",
    "               avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2-2*h, dd3=dd3,vv=vv))/(4*h*h))\n",
    "    df_dd3_dd3=((avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2, dd3=dd3+2*h,vv=vv)-2*ff+\n",
    "               avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2, dd3=dd3-2*h,vv=vv))/(4*h*h))\n",
    "    \n",
    "    df_lr_dd1=((avg_evaluate_f(lr=lr+k, dd1=dd1+h, dd2=dd2, dd3=dd3,vv=vv)-\n",
    "                avg_evaluate_f(lr=lr+k, dd1=dd1-h, dd2=dd2, dd3=dd3,vv=vv)-\n",
    "                avg_evaluate_f(lr=lr-k, dd1=dd1+h, dd2=dd2, dd3=dd3,vv=vv)+\n",
    "                avg_evaluate_f(lr=lr-k, dd1=dd1-h, dd2=dd2, dd3=dd3,vv=vv))/(4*k*h))\n",
    "    df_lr_dd2=((avg_evaluate_f(lr=lr+k, dd1=dd1, dd2=dd2+h, dd3=dd3,vv=vv)-\n",
    "                avg_evaluate_f(lr=lr+k, dd1=dd1, dd2=dd2-h, dd3=dd3,vv=vv)-\n",
    "                avg_evaluate_f(lr=lr-k, dd1=dd1, dd2=dd2+h, dd3=dd3,vv=vv)+\n",
    "                avg_evaluate_f(lr=lr-k, dd1=dd1, dd2=dd2-h, dd3=dd3,vv=vv))/(4*k*h))\n",
    "    df_lr_dd3=((avg_evaluate_f(lr=lr+k, dd1=dd1, dd2=dd2, dd3=dd3+h,vv=vv)-\n",
    "                avg_evaluate_f(lr=lr+k, dd1=dd1, dd2=dd2, dd3=dd3-h,vv=vv)-\n",
    "                avg_evaluate_f(lr=lr-k, dd1=dd1, dd2=dd2, dd3=dd3+h,vv=vv)+\n",
    "                avg_evaluate_f(lr=lr-k, dd1=dd1, dd2=dd2, dd3=dd3-h,vv=vv))/(4*k*h))\n",
    "    \n",
    "    df_dd1_dd2=((avg_evaluate_f(lr=lr, dd1=dd1+h, dd2=dd2+h, dd3=dd3,vv=vv)-\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1+h, dd2=dd2-h, dd3=dd3,vv=vv)-\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1-h, dd2=dd2+h, dd3=dd3,vv=vv)+\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1-h, dd2=dd2-h, dd3=dd3,vv=vv))/(4*h*h))\n",
    "    df_dd1_dd3=((avg_evaluate_f(lr=lr, dd1=dd1+h, dd2=dd2, dd3=dd3+h,vv=vv)-\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1+h, dd2=dd2, dd3=dd3-h,vv=vv)-\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1-h, dd2=dd2, dd3=dd3+h,vv=vv)+\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1-h, dd2=dd2, dd3=dd3-h,vv=vv))/(4*h*h))\n",
    "    \n",
    "    df_dd2_dd3=((avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2+h, dd3=dd3+h,vv=vv)-\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2+h, dd3=dd3-h,vv=vv)-\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2-h, dd3=dd3+h,vv=vv)+\n",
    "                 avg_evaluate_f(lr=lr, dd1=dd1, dd2=dd2-h, dd3=dd3-h,vv=vv))/(4*h*h))\n",
    "    \n",
    "    \n",
    "    H=numpy.array([[df_lr_lr ,df_lr_dd1 ,df_lr_dd2 ,df_lr_dd3],\n",
    "                   [df_lr_dd1,df_dd1_dd1,df_dd1_dd2,df_dd1_dd3],\n",
    "                   [df_lr_dd2,df_dd1_dd2,df_dd2_dd2,df_dd2_dd3],\n",
    "                   [df_lr_dd3,df_dd1_dd3,df_dd2_dd3,df_dd3_dd3]],dtype=numpy.float)\n",
    "    #HH=numpy.reshape(H,(4,4))\n",
    "\n",
    "    return H\n",
    "\n",
    "def evaluate_H_v(ff,VV):\n",
    "    val=evaluate_H(ff,lr=VV[0], dd1=VV[1], dd2=VV[2], dd3=VV[3], vv=0)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_model(pp,ff,GG,HH):\n",
    "    RR=ff+pp@GG +(0.5*(pp@HH@pp))\n",
    "    return RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_vec(mu,stp_dir,full_step):\n",
    "    return mu[0]*stp_dir+mu[1]*full_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_sub_space(stp,fu,HH,GG):\n",
    "    A=numpy.array([[stp @ HH @ stp, stp @ HH @ fu],\n",
    "                   [fu  @ HH @ stp, fu  @ HH @ fu]],dtype=numpy.float)\n",
    "    b=numpy.array([stp @GG ,fu @ GG],dtype=numpy.float)\n",
    "    return -1*numpy.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_search(sigma,vec1,vec2,ff,GG,HH,rad):\n",
    "    kk=rad*(numpy.cos(sigma)*vec1+numpy.sin(sigma)*vec2)\n",
    "    return q_model(kk,ff,GG,HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vec(sigma,vec1,vec2,rad):\n",
    "    kk=rad*(numpy.cos(sigma)*vec1+numpy.sin(sigma)*vec2)\n",
    "    return kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_base(stp,fu):\n",
    "    q1=stp/LA.norm(stp)\n",
    "    inn=fu-numpy.inner(stp,fu)*q1\n",
    "    q2=inn/LA.norm(inn)\n",
    "    \n",
    "    return (q1,q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(beta,A):\n",
    "    I=numpy.identity(4)\n",
    "    HH=A+beta*I\n",
    "    return numpy.linalg.cond(HH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = numpy.empty((0,4), dtype=numpy.float)\n",
    "path_grad=numpy.empty((0,4), dtype=numpy.float)\n",
    "zero_arr=numpy.array([0,0,0,0])\n",
    "rad=0.001\n",
    "max_rad=0.1\n",
    "tol=1/4\n",
    "GG=1\n",
    "XX=numpy.array([0.001,0.2,0.2,0.2])\n",
    "tt=1\n",
    "rho_count=0\n",
    "#cat_n.reshape((1,cat_n.shape[0]))\n",
    "path = numpy.append(path,XX.reshape((1,XX.shape[0])) , axis=0)\n",
    "while ((LA.norm(GG)>0.0001) and (tt<40)):\n",
    "    print('itt= ',tt,file=f1)\n",
    "    print('evaluating function',file=f1)\n",
    "    FF=avg_evaluate_f_v(XX)\n",
    "    print('done ',FF,file=f1)\n",
    "    print('evaluating gradient',file=f1)\n",
    "    GG=evaluate_G_v(XX)\n",
    "    path_grad=numpy.append(path_grad,GG.reshape((1,GG.shape[0])) , axis=0)\n",
    "    if(LA.norm(GG)<0.0001):\n",
    "        break\n",
    "    print('evaluating hessian',file=f1)\n",
    "    HH=evaluate_H_v(FF,XX)\n",
    "    print('done',file=f1)\n",
    "    print('getting eigenvalues',file=f1)\n",
    "    min_e=numpy.real(numpy.min(LA.eigvals(HH)))\n",
    "    \n",
    "    print('min eig =',  min_e,file=f1)\n",
    "    if min_e < 0:\n",
    "        (kk1,kk2,kk3,kk4)=OPT(condition,-1.1*min_e,-2*min_e,args=(HH,),full_output=1)\n",
    "        print(kk1,kk2,kk3,kk4)\n",
    "        print(kk1/min_e)\n",
    "        HH=HH+kk1*numpy.identity(4)\n",
    "    print('solving subspace',file=f1)\n",
    "    full_step=-1*numpy.linalg.solve(HH,GG)\n",
    "    stp_dir=-1*((GG @ GG)/(GG @ HH @ GG))*GG\n",
    "    kk=solve_sub_space(stp_dir,full_step,HH,GG)\n",
    "    pp=sub_vec(kk,stp_dir,full_step)\n",
    "    t_pp=XX+pp\n",
    "    (q1,q2)=change_base(stp_dir,full_step)\n",
    "    if ((LA.norm(pp)>rad) or (numpy.min(t_pp)<0) or (numpy.max(t_pp[1:])>1)):\n",
    "        dic= (q1,q2,FF,GG,HH,rad)\n",
    "        (opt1,opt2,opt3,opt4)=OPT(one_search,0,2*numpy.pi,args=dic,full_output=1)\n",
    "        pp=one_vec(opt1,q1,q2,rad)\n",
    "    FF_n=avg_evaluate_f_v(XX+pp)\n",
    "    rho_top=FF-FF_n\n",
    "    rho_bot=q_model(zero_arr,FF,GG,HH)-q_model(pp,FF,GG,HH)\n",
    "    if (rho_bot<10*numpy.finfo(float).eps):\n",
    "        print('rho_bot too close to epsilon',file=f1)\n",
    "        rad=0.001\n",
    "        rho_count=rho_count+1\n",
    "        continue\n",
    "    rho=rho_top/rho_bot\n",
    "    while ((rho < 1/4) or (LA.norm(pp)>rad) or (numpy.min(t_pp)<0) or (numpy.max(t_pp[1:])>1)):\n",
    "        rad=rad*1/4\n",
    "        dic= (q1,q2,FF,GG,HH,rad)\n",
    "        (opt1,opt2,opt3,opt4)=OPT(one_search,0,2*numpy.pi,args=dic,full_output=1)\n",
    "        pp=one_vec(kk1,q1,q2,rad)\n",
    "        FF_n=avg_evaluate_f_v(XX+pp)\n",
    "        rho_top=FF-FF_n\n",
    "        rho_bot=q_model(zero_arr,FF,GG,HH)-q_model(pp,FF,GG,HH)\n",
    "        if (rho_bot<10*numpy.finfo(float).eps):\n",
    "            print('rho_bot too close to epsilon',file=f1)\n",
    "            rad=0.001\n",
    "            rho_count=rho_count+1\n",
    "            break\n",
    "        rho=rho_top/rho_bot\n",
    "    if ((rho>3/4) and (LA.norm(pp)<1.001*rad)):\n",
    "        rad=min(2*rad,max_rad)\n",
    "    XX=XX+pp\n",
    "    tt=tt+1\n",
    "    print('final radius ',rad,file=f1)\n",
    "    print('grad norm ',LA.norm(GG),file=f1)\n",
    "    path = numpy.append(path,XX.reshape((1,XX.shape[0])) , axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)\n",
    "print(path_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(path, open( \"path.p\", \"wb\" ) )\n",
    "pickle.dump(path_grad, open( \"path_grad.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX2=numpy.array([0.001,0.2,0.2,0.2])\n",
    "#GG_i=GG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FF=avg_evaluate_f_v(XX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GG_i=evaluate_G_v(XX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "HH=evaluate_H_v(FF,XX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model('di_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_pred=model.predict(test_X)\n",
    "#numpy.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(arr_pred-test_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=pickle.load(open( \"2_path.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XX[0,...],' -> ',avg_evaluate_f_acc_v(XX[0,...]))\n",
    "print(XX[29,...],' -> ',avg_evaluate_f_acc_v(XX[29,...]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
